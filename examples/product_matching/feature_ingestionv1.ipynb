{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This file performs feature ingestion from feature files. Once the migration of feature storage within\n",
    "# product matching is complete it will be possible to migrate this notebook to obtain the features\n",
    "# directly from BigQuery. Currently this needs msgpack and msgpack_numpy installed. These will be removed once\n",
    "# data is obtained from BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import datetime as dt\n",
    "from pytz import utc\n",
    "\n",
    "from google.cloud import storage as gcs\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import msgpack\n",
    "import msgpack_numpy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from feast import Client, FeatureSet, Entity, ValueType, Feature\n",
    "from google.protobuf.duration_pb2 import Duration\n",
    "from random import randrange, randint\n",
    "\n",
    "from feast.types.Value_pb2 import DoubleList as FeatureDoubleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_name = 'product_matching_team'\n",
    "feature_set_name = 'product_features'\n",
    "\n",
    "preprocessing_system_version = '0.5.0'\n",
    "bucket_name = 'mount-pm-matching-production-features'\n",
    "markets = ['US']\n",
    "scrape_date_from = '2019-01-16'\n",
    "scrape_date_to = '2019-01-31'\n",
    "sites = ['farfetch']\n",
    "\n",
    "FEATURE_NAMES = ['texture_embedding', 'spoc_embedding', 'local_histogram', 'ccv']\n",
    "FEATURE_TYPES = [ValueType.DOUBLE_LIST, ValueType.DOUBLE_LIST, ValueType.DOUBLE_LIST, ValueType.DOUBLE_LIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CORE_URL = os.getenv('FEAST_CORE_URL')\n",
    "BATCH_SERVING_URL = os.getenv('FEAST_BATCH_SERVING_URL')\n",
    "ONLINE_SERVING_URL = os.getenv('FEAST_ONLINE_SERVING_URL')\n",
    "print(CORE_URL, BATCH_SERVING_URL, ONLINE_SERVING_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_project(project_name):\n",
    "    client = Client(core_url=CORE_URL, serving_url=BATCH_SERVING_URL, project=project_name)\n",
    "    if project_name not in client.list_projects():\n",
    "        print('constructing project: {}'.format(project_name))\n",
    "        client.create_project(project_name)\n",
    "    else:\n",
    "        print('project already exists: {}'.format(project_name))\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_feature_set(client, project_name, feature_set_name, feature_names, feature_types, max_age=365*86400):\n",
    "    feature_sets = client.list_feature_sets(\n",
    "    project=project_name, \n",
    "    name=feature_set_name)\n",
    "    \n",
    "    if feature_sets:\n",
    "        print('feature set already exists: {}'.format(feature_set_name))\n",
    "        return feature_sets[0]\n",
    "    else:\n",
    "        # update to define schema\n",
    "        feature_set = FeatureSet(\n",
    "            feature_set_name,\n",
    "            max_age=Duration(seconds=max_age), \n",
    "            entities=[Entity(name='product_id', dtype=ValueType.INT64)],\n",
    "            features=[Feature(name=name, dtype=dtype) for name, dtype in zip(feature_names, feature_types)] \n",
    "        )\n",
    "        client.apply(feature_set)\n",
    "        return feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = construct_project(project_name=project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_set = construct_feature_set(client, project_name, feature_set_name, FEATURE_NAMES, FEATURE_TYPES, max_age=365*86400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print([x.name for x in feature_set.entities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print([x.name for x in feature_set.features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATE_FORMAT = '%Y-%m-%d'\n",
    "\n",
    "SITES = (\n",
    "    'aizel',\n",
    "    'alexandermcqueen',\n",
    "    'balenciaga',\n",
    "    'balmain',\n",
    "    'bergdorfgoodman',\n",
    "    'bloomingdales',\n",
    "    'brownsfashion',\n",
    "    'burberry',\n",
    "    'chloe',\n",
    "    'dolceandgabbana',\n",
    "    'dsquared2',\n",
    "    'farfetch',\n",
    "    'fendi',\n",
    "    'fwrd',\n",
    "    'gucci',\n",
    "    'harrods',\n",
    "    'jimmychoo',\n",
    "    'kenzo',\n",
    "    'luisaviaroma',\n",
    "    'marni',\n",
    "    'matchesfashion',\n",
    "    'miumiu',\n",
    "    'moncler',\n",
    "    'mrporter',\n",
    "    'msgm',\n",
    "    'mytheresa',\n",
    "    'neimanmarcus',\n",
    "    'netaporter',\n",
    "    'nordstrom',\n",
    "    'offwhite',\n",
    "    'ounass',\n",
    "    'prada',\n",
    "    'redone',\n",
    "    'rickowens',\n",
    "    'saintlaurent',\n",
    "    'saksfifthavenue',\n",
    "    'secoo',\n",
    "    'selfportrait',\n",
    "    'selfridges',\n",
    "    'shopbop',\n",
    "    'ssense',\n",
    "    'stylebop',\n",
    "    'stellamccartney',\n",
    "    'thombrowne',\n",
    "    'tmall',\n",
    "    'toryburch',\n",
    "    'tsum',\n",
    "    'twentyfoursevres',\n",
    "    'valentino',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Storage(BaseGoogleClient):\n",
    "    \"\"\"Google Cloud Storage client.\"\"\"\n",
    "    client_factory = gcs.Client\n",
    "\n",
    "    def blob_exists(self, bucket_name, blob_name):\n",
    "        \"\"\"\n",
    "        Simple wrapper function to determine whether a blob_name exists on the given GCS bucket.\n",
    "        :param bucket_name: GCS bucket name.\n",
    "        :param blob_name: GCS blob name.\n",
    "        :return: Boolean\n",
    "        \"\"\"\n",
    "        return self.blob(bucket_name, blob_name).exists()\n",
    "\n",
    "    def save_to_json(self, obj, bucket_name, blob_name):\n",
    "        \"\"\"JSON Serializes a dict and stores it in GCS in the bucket/name provided.\n",
    "\n",
    "        :param dict obj: A JSON serializable dict.\n",
    "        :param str bucket_name: GCS bucket name.\n",
    "        :param str blob_name: GCS blob name.\n",
    "        \"\"\"\n",
    "        blob = self.blob(bucket_name, blob_name)\n",
    "\n",
    "        with io.BytesIO() as f:\n",
    "            f.write(json.dumps(obj).encode())\n",
    "            blob.upload_from_file(f, rewind=True)\n",
    "\n",
    "    def load_json(self, bucket_name, blob_name):\n",
    "        \"\"\"Downloads and deserializes a JSON file from the bucket/name provided.\n",
    "\n",
    "        :param str bucket_name: GCS bucket name.\n",
    "        :param str blob_name: GCS blob name.\n",
    "        :returns: A dict of the deserialized JSON file.\n",
    "        \"\"\"\n",
    "        with self.bytesio_blob(bucket_name, blob_name) as f:\n",
    "            obj = json.loads(f.getvalue().decode())\n",
    "\n",
    "        return obj\n",
    "\n",
    "    def bucket(self, bucket_name):\n",
    "        return self.client.get_bucket(bucket_name)\n",
    "\n",
    "    def blob(self, bucket_name, blob_name):\n",
    "        return self.bucket(bucket_name).blob(blob_name)\n",
    "\n",
    "    @contextmanager\n",
    "    def bytesio_blob(self, bucket_name, blob_name):\n",
    "        \"\"\"\n",
    "        Returns the contents of the blob as BytesIO in the target of a context manager\n",
    "        :param str bucket_name:\n",
    "        :param str blob_name:\n",
    "        \"\"\"\n",
    "        blob = self.blob(bucket_name, blob_name)\n",
    "        with io.BytesIO() as f:\n",
    "            blob.download_to_file(f)\n",
    "            f.seek(0)\n",
    "            yield f\n",
    "\n",
    "    @contextmanager\n",
    "    def tmpfile_blob(self, bucket_name, blob_name):\n",
    "        \"\"\"\n",
    "        Returns the contents of the blob as temporary file descriptor in the target of a context manager\n",
    "        :param str bucket_name:\n",
    "        :param str blob_name:\n",
    "        \"\"\"\n",
    "        blob = self.blob(bucket_name, blob_name)\n",
    "        with tempfile.NamedTemporaryFile() as f:\n",
    "            blob.download_to_filename(f.name)\n",
    "            yield f\n",
    "\n",
    "    def explode_tgz(self, bucket_name, blob_name, target_dir):\n",
    "        \"\"\"\n",
    "        Downloads .tgz file from GCS and extracts contents into target directory\n",
    "        :param bucket_name:\n",
    "        :param blob_name:\n",
    "        :param target_dir: file system path into which to extract contents of archive\n",
    "        \"\"\"\n",
    "        with tempfile.TemporaryDirectory() as d:\n",
    "            tgz_filename = os.path.join(d, 'temp.tgz')\n",
    "\n",
    "            with open(tgz_filename, 'wb') as f:\n",
    "                self.blob(bucket_name, blob_name).download_to_file(f)\n",
    "\n",
    "            with tarfile.open(tgz_filename, 'r:gz') as tgz:\n",
    "                tgz.extractall(path=target_dir)\n",
    "\n",
    "    def download_uris_from_file(self, uris_filename, target_dir):\n",
    "        \"\"\"\n",
    "        The contents for the file of uris should not contain duplicates to avoid problems with GCS downloads blocking.\n",
    "\n",
    "        gsutil is expected to be available where this method is executing.\n",
    "\n",
    "        gsutil can and does fail to download files from GCS on occasion which is why we retry.  To prevent repeatedly\n",
    "        trying to download the entire list of uris, this list is instead filtered by files corresponding to blobs\n",
    "        already downloaded.  This is the responsibility of the caller when scripting multi-processing gsutil commands:\n",
    "\n",
    "        https://cloud.google.com/storage/docs/gsutil/addlhelp/ScriptingProductionTransfers\n",
    "\n",
    "        :param uris_filename: path to file containing one uri per line\n",
    "        :param target_dir: directory into which files corresponding to the uris will be copied\n",
    "        :raises: GCSDownloadException\n",
    "        \"\"\"\n",
    "        print('Downloading blobs from uris listed in file %s to %s', uris_filename, target_dir)\n",
    "        cmd_args = ['gsutil', '-mq', 'cp', '-I', target_dir]\n",
    "\n",
    "        download_filename = '{}.downloading'.format(uris_filename)\n",
    "        with open(download_filename, 'w') as f_out:\n",
    "            with open(uris_filename, 'r') as f_in:\n",
    "                for l in f_in:\n",
    "                    target_filename = os.path.join(target_dir, l.strip().split('/')[-1])\n",
    "                    if not os.path.exists(target_filename):\n",
    "                        f_out.write(l)\n",
    "        try:\n",
    "            with open(download_filename, 'r') as stdin:\n",
    "                subprocess.run(cmd_args, stdin=stdin, shell=False, check=True)\n",
    "        except subprocess.CalledProcessError as ex:\n",
    "            msg = 'Unable to download images from file {} to {} after retries'.format(uris_filename, target_dir)\n",
    "            raise errors.GCSDownloadException(msg) from ex\n",
    "        finally:\n",
    "            if os.path.exists(download_filename):\n",
    "                os.remove(download_filename)\n",
    "\n",
    "    def copy(self, source, target, is_recursive=False):\n",
    "        \"\"\"\n",
    "        Uses gsutils to copy from source to target\n",
    "        :param source: local file system path or gs uri\n",
    "        :param target: local file system path or gs uri\n",
    "        :param is_recursive: if copy is to be recursive\n",
    "        :raises errors.GCSDownloadException:\n",
    "        \"\"\"\n",
    "        print('Copying files from %s to %s', source, target)\n",
    "\n",
    "        cmd_args = ['gsutil', '-mq', 'cp']\n",
    "        if is_recursive:\n",
    "            cmd_args.append('-r')\n",
    "\n",
    "        cmd_args.extend([source, target])\n",
    "\n",
    "        try:\n",
    "            subprocess.run(cmd_args, shell=False, check=True)\n",
    "        except subprocess.CalledProcessError as ex:\n",
    "            msg = 'Unable to upload from {} to {}'.format(source, target)\n",
    "            raise errors.GCSDownloadException(msg) from ex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date_iterator(date_from, date_to):\n",
    "    \"\"\"\n",
    "    Iterate over the given date range.\n",
    "    :param date_from: The initial date of the date window.\n",
    "    :type date_from: datetime.datetime\n",
    "    :param date_to: The initial date of the date window.\n",
    "    :type date_to: datetime.datetime\n",
    "    :return: yields a string representation of the current date.\n",
    "    \"\"\"\n",
    "    while date_from <= date_to:\n",
    "        yield date_from.strftime(DATE_FORMAT)\n",
    "        date_from += dt.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_date_range(date_string=None, date_from_string=None, date_to_string=None, n_days=None):\n",
    "    \"\"\"\n",
    "    Construct a date range.\n",
    "    Note: The date strings are expected to be in the format given by, settings.DATE_FORMAT.\n",
    "    :param date_string: A string, denoting a date, or None.\n",
    "    :param date_from_string: A string, denoting a date, or None.\n",
    "    :param date_to_string: A string, denoting a date, or None.\n",
    "    :param n_days: A positive integer or None. If given this is the size of the date range\n",
    "    :return: A list of strings, each denoting a date.\n",
    "    \"\"\"\n",
    "    if date_string is not None:\n",
    "        if n_days is None:\n",
    "            yield date_string\n",
    "        else:\n",
    "            if n_days >= 0:\n",
    "                date_from = dt.datetime.strptime(date_string, DATE_FORMAT)\n",
    "                date_to = date_from + dt.timedelta(days=n_days - 1)\n",
    "            else:\n",
    "                date_to = dt.datetime.strptime(date_string, DATE_FORMAT)\n",
    "                date_from = date_to - dt.timedelta(days=abs(n_days) - 1)\n",
    "            yield from date_iterator(date_from, date_to)\n",
    "\n",
    "    else:\n",
    "        if not date_from_string:\n",
    "            return\n",
    "\n",
    "        date_from = dt.datetime.strptime(date_from_string, DATE_FORMAT)\n",
    "        if date_to_string is None:\n",
    "            date_to = dt.datetime.today()\n",
    "        else:\n",
    "            date_to = dt.datetime.strptime(date_to_string, DATE_FORMAT)\n",
    "        yield from date_iterator(date_from, date_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_features_blob_name(preprocessing_system_version, site, market, scrape_date):\n",
    "    \"\"\"\n",
    "    Construct a file path for the pre-processed product features.\n",
    "    :param preprocessing_system_version: The version of the preprocessing system\n",
    "    :param site: The site of the products\n",
    "    :param market: The market of the products\n",
    "    :param scrape_date: The scrape date of the products\n",
    "    :return: The file path of the file for the pre-processed product image features.\n",
    "    \"\"\"\n",
    "    return os.path.join(preprocessing_system_version, site, market, scrape_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_features_blob_names(\n",
    "        preprocessing_system_version, markets, scrape_date_from, scrape_date_to=None, sites=None):\n",
    "    \"\"\"\n",
    "    Construct a list of file paths for the pre-processed product features.\n",
    "    :param preprocessing_system_version: The version of the preprocessing system\n",
    "    :param markets: The market(s) of the products\n",
    "    :param scrape_date_from: The lower bound on the scrape date of the products\n",
    "    :param scrape_date_to: The upper bound on the scrape date of the products\n",
    "    :param sites: The sites of the products\n",
    "    :return: The file paths of the file for the pre-processed product image features.\n",
    "    \"\"\"\n",
    "    markets = [markets] if isinstance(markets, str) else markets\n",
    "    if sites is None:\n",
    "        sites = constants.SITES\n",
    "    scrape_dates = construct_date_range(date_from_string=scrape_date_from, date_to_string=scrape_date_to)\n",
    "\n",
    "    return [construct_features_blob_name(preprocessing_system_version, s, market, sd)\n",
    "            for sd in scrape_dates\n",
    "            for s in sites\n",
    "            for market in markets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_features_from_gcs(\n",
    "    preprocessing_system_version,\n",
    "    storage,\n",
    "    bucket_name,\n",
    "    markets,\n",
    "    scrape_date_from,\n",
    "    scrape_date_to=None,\n",
    "    sites=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generator of product features from all files matching the given criteria\n",
    "    :param preprocessing_system_version:\n",
    "    :param product_matching_base.clients.google.Storage storage:\n",
    "    :param bucket_name: GCS bucket name\n",
    "    :param markets: List of markets or single str of market code.\n",
    "    :param scrape_date_from: str in format %Y-%m-%d e.g. 2018-01-01\n",
    "    :param scrape_date_to: (optional) str in format %Y-%m-%d e.g. 2018-01-01, today if not specified\n",
    "    :param sites: list of str, if not specified constants.SITES\n",
    "    :yields: dicts with features including natural keys of products.\n",
    "    \"\"\"\n",
    "    markets = [markets] if isinstance(markets, str) else markets\n",
    "    for market in markets:\n",
    "        blob_names = construct_features_blob_names(\n",
    "            preprocessing_system_version,\n",
    "            market,\n",
    "            scrape_date_from,\n",
    "            scrape_date_to=scrape_date_to,\n",
    "            sites=sites,\n",
    "        )\n",
    "        for blob_name in blob_names:\n",
    "            if storage.blob_exists(bucket_name, blob_name):\n",
    "                yield from load_features_from_blob(storage, bucket_name, blob_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_features_from_blob(storage, bucket_name, blob_name):\n",
    "    \"\"\"\n",
    "    Downloads blob from Google Cloud Storage and yields product feature dicts\n",
    "    :param product_matching_base.clients.google.Storage storage:\n",
    "    :param bucket_name: name of GCS bucket\n",
    "    :param blob_name: name of GCS blob\n",
    "    \"\"\"\n",
    "    with storage.tmpfile_blob(bucket_name, blob_name) as f:\n",
    "        yield from msgpack.Unpacker(f, object_hook=msgpack_numpy.decode, raw=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_product_id(feature_dict):\n",
    "    return int(feature_dict['site_product_id'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "storage = Storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ingest_features_to_store(feature_set, storage, bucket_name, markets, scrape_date_from, scrape_date_to, sites, preprocessing_system_version):\n",
    "    features = list(load_features_from_gcs(\n",
    "    preprocessing_system_version,\n",
    "    storage,\n",
    "    bucket_name,\n",
    "    markets,\n",
    "    scrape_date_from,\n",
    "    scrape_date_to,\n",
    "    sites=sites))\n",
    "    \n",
    "    print('Number of features for date {0}: {1}'.format(scrape_date_from, len(features)))\n",
    "    \n",
    "    product_ids = [construct_product_id(x) for x in features]\n",
    "    product_features = pd.DataFrame(\n",
    "        {\n",
    "            \"datetime\": [dt.datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0).replace(tzinfo=utc) for _ in range(len(features))],\n",
    "            \"product_id\": product_ids,\n",
    "            \"texture_embedding\": [x['texture_embedding'].squeeze() for x in features],\n",
    "            \"spoc_embedding\": [x['spoc'].squeeze() for x in features],\n",
    "            \"local_histogram\": [np.hstack(x['local_histogram']) for x in features],\n",
    "            \"ccv\": [np.hstack(x['ccv']) for x in features],\n",
    "        })\n",
    "    \n",
    "    if product_features.shape[0] > 0:\n",
    "        client.ingest(feature_set, product_features, chunk_size=10)\n",
    "\n",
    "    return product_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_store_keys(storage, bucket_name, markets, scrape_date_from, scrape_date_to, sites, preprocessing_system_version):\n",
    "    features = list(load_features_from_gcs(\n",
    "    preprocessing_system_version,\n",
    "    storage,\n",
    "    bucket_name,\n",
    "    markets,\n",
    "    scrape_date_from,\n",
    "    scrape_date_to,\n",
    "    sites=sites))\n",
    "    \n",
    "    print('Number of features for date {0}: {1}'.format(scrape_date_from, len(features)))\n",
    "    \n",
    "    return [construct_product_id(x) for x in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stored_product_keys = []\n",
    "\n",
    "for scrape_date in construct_date_range(date_from_string=scrape_date_from, date_to_string=scrape_date_to):\n",
    "    stored_product_keys.extend(ingest_features_to_store(\n",
    "        feature_set,\n",
    "        storage,\n",
    "        bucket_name,\n",
    "        markets,\n",
    "        scrape_date_from=scrape_date,\n",
    "        scrape_date_to=scrape_date,\n",
    "        sites=sites,\n",
    "        preprocessing_system_version=preprocessing_system_version))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
